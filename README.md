# MUVO: è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆçš„è‡ªåŠ¨é©¾é©¶å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
# MUVO: Cross-Modal Attention Fusion for Autonomous Driving Anomaly Detection

## ğŸš€ æ ¸å¿ƒåˆ›æ–° (Core Innovation)

æœ¬é¡¹ç›®åœ¨åŸæœ‰MUVOåŸºç¡€ä¸Šå®ç°äº†**è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆçš„å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ**ï¼Œä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

### 1. å†»ç»“éª¨å¹²ç½‘ç»œæ¶æ„ (Frozen Backbone Architecture)
- **å›¾åƒåˆ†æ”¯**: ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet18ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œæƒé‡å†»ç»“
- **ç‚¹äº‘åˆ†æ”¯**: ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet18å¤„ç†range-viewç‚¹äº‘ï¼Œæƒé‡å†»ç»“
- **ä¼˜åŠ¿**: å¤§å¹…å‡å°‘è®­ç»ƒæ—¶é—´å’Œæ˜¾å­˜æ¶ˆè€—ï¼Œæé«˜è®­ç»ƒæ•ˆç‡

### 2. è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ (Cross-Modal Attention Fusion) â­
- **æ ¸å¿ƒåˆ›æ–°**: è½»é‡çº§è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—
- **Query**: ç‚¹äº‘ç‰¹å¾ (Point Cloud Features)
- **Key & Value**: å¯¹é½åçš„å›¾åƒç‰¹å¾ (Aligned Image Features)
- **è¾“å‡º**: å¢å¼ºçš„ç‚¹äº‘ç‰¹å¾ï¼Œèåˆäº†å›¾åƒçº¹ç†ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯

### 3. ç‰¹å¾ç©ºé—´å¯¹é½ (Feature Space Alignment)
- æ ¹æ®æŠ•å½±å…³ç³»å°†å›¾åƒç‰¹å¾æ˜ å°„åˆ°ä½“ç´ ç©ºé—´
- æ”¯æŒå¤šç§å¯¹é½æ–¹æ³•ï¼ˆæœ€è¿‘é‚»ã€åŒçº¿æ€§æ’å€¼ç­‰ï¼‰
- ç¡®ä¿è·¨æ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆ

### 4. è½»é‡çº§å¼‚å¸¸æ£€æµ‹å¤´ (Lightweight Anomaly Detection Head)
- **3D CNN**: è½»é‡çº§3Då·ç§¯ç½‘ç»œ
- **MLP**: å¤šå±‚æ„ŸçŸ¥æœºæ¶æ„
- **å¤šå°ºåº¦**: å¤šå°ºåº¦ç‰¹å¾èåˆ
- **ä»…æ­¤éƒ¨åˆ†å¯è®­ç»ƒ**: å…¶ä»–éª¨å¹²ç½‘ç»œæƒé‡å†»ç»“

## ğŸ“Š ç³»ç»Ÿæ¶æ„ (System Architecture)

```
Stage 1: è¾“å…¥ä¸é¢„å¤„ç†
â”œâ”€â”€ å›¾åƒ (HÃ—WÃ—3)
â”œâ”€â”€ ç‚¹äº‘ (NÃ—4) 
â””â”€â”€ æ ‡å®šå‚æ•°

Stage 2: å†»ç»“éª¨å¹²ç½‘ç»œç‰¹å¾æå–
â”œâ”€â”€ å›¾åƒåˆ†æ”¯ (ResNet18, æƒé‡å†»ç»“)
â””â”€â”€ ç‚¹äº‘åˆ†æ”¯ (ResNet18, æƒé‡å†»ç»“)

Stage 3: â­ æ ¸å¿ƒåˆ›æ–° - è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ
â”œâ”€â”€ ç‰¹å¾ç©ºé—´å¯¹é½
â”œâ”€â”€ è½»é‡çº§è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—
â””â”€â”€ å¢å¼ºçš„ç‚¹äº‘ç‰¹å¾

Stage 4: å¼‚å¸¸æ£€æµ‹å¤´ä¸è¾“å‡º
â”œâ”€â”€ è½»é‡çº§3D CNN/MLP (ä»…æ­¤éƒ¨åˆ†å¯è®­ç»ƒ)
â””â”€â”€ ä½“ç´ çº§å¼‚å¸¸åˆ†æ•° & å¼‚å¸¸çƒ­åŠ›å›¾
```

## ğŸ› ï¸ æŠ€æœ¯ç‰¹ç‚¹ (Technical Features)

- **é«˜æ•ˆè®­ç»ƒ**: éª¨å¹²ç½‘ç»œå†»ç»“ï¼Œä»…è®­ç»ƒå¼‚å¸¸æ£€æµ‹å¤´
- **è·¨æ¨¡æ€èåˆ**: å›¾åƒå’Œç‚¹äº‘ç‰¹å¾çš„æ·±åº¦äº¤äº’
- **è½»é‡çº§è®¾è®¡**: è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆå®æ—¶åº”ç”¨
- **æ¨¡å—åŒ–æ¶æ„**: æ˜“äºæ‰©å±•å’Œä¿®æ”¹
- **å¤šå°ºåº¦æ£€æµ‹**: æ”¯æŒä¸åŒå°ºåº¦çš„å¼‚å¸¸æ£€æµ‹

---

## åŸå§‹é¡¹ç›®è¯´æ˜ (Original Project Description)

This is the PyTorch implementation for the paper
>  Occupancy-Guided Sensor Fusion Strategies for Generative Predictive World Models <br/>

## Requirements
The simplest way to install all required dependencies is to create 
a [conda](https://docs.conda.io/projects/miniconda/en/latest/) environment by running
```
conda env create -f carla_env.yml
```
Then activate conda environment by
```
conda activate muvo
```
or create your own venv and install the requirement by running
```
pip install -r requirements.txt
```


## Dataset
Use [CARLA](http://carla.org/) to collection data. 
First install carla refer to its [documentation](https://carla.readthedocs.io/en/latest/).

### Dataset Collection
Change settings in config/, 
then run `bash run/data_collect.sh ${PORT}` 
with `${PORT}` the port to run CARLA (usually `2000`) <br/>
The data collection code is modified from 
[CARLA-Roach](https://github.com/zhejz/carla-roach) and [MILE](https://github.com/wayveai/mile),
some config settings can be referred there.

### Voxelization
After collecting the data by CARLA, create voxels data by running `data/generate_voxels.py`, <br/> 
voxel settings can be changed in `data_preprocess.yaml`.

### Folder Structure
After completing the above steps, or otherwise obtaining the dataset,
please change the file structure of the dataset. <br/>

The main branch includes most of the results presented in the paper. In the 2D branch, you can find 2D latent states, perceptual losses, and a new transformer backbone. The data is organized in the following format
```
/carla_dataset/trainval/
                   â”œâ”€â”€ train/
                   â”‚     â”œâ”€â”€ Town01/
                   â”‚     â”‚     â”œâ”€â”€ 0000/
                   â”‚     â”‚     â”‚     â”œâ”€â”€ birdview/
                   â”‚     â”‚     â”‚     â”‚      â”œ birdview_000000000.png
                   â”‚     â”‚     â”‚     â”‚      .
                   â”‚     â”‚     â”‚     â”œâ”€â”€ depth_semantic/
                   â”‚     â”‚     â”‚     â”‚      â”œ depth_semantic_000000000.png
                   â”‚     â”‚     â”‚     â”‚      .
                   â”‚     â”‚     â”‚     â”œâ”€â”€ image/
                   â”‚     â”‚     â”‚     â”‚      â”œ image_000000000.png
                   â”‚     â”‚     â”‚     â”‚      .
                   â”‚     â”‚     â”‚     â”œâ”€â”€ points/
                   â”‚     â”‚     â”‚     â”‚      â”œ points_000000000.png
                   â”‚     â”‚     â”‚     â”‚      .
                   â”‚     â”‚     â”‚     â”œâ”€â”€ points_semantic/
                   â”‚     â”‚     â”‚     â”‚      â”œ points_semantic_000000000.png
                   â”‚     â”‚     â”‚     â”‚      .
                   â”‚     â”‚     â”‚     â”œâ”€â”€ routemap/
                   â”‚     â”‚     â”‚     â”‚      â”œ routemap_000000000.png
                   â”‚     â”‚     â”‚     â”‚      .
                   â”‚     â”‚     â”‚     â”œâ”€â”€ voxel/
                   â”‚     â”‚     â”‚     â”‚      â”œ voxel_000000000.png
                   â”‚     â”‚     â”‚     â”‚      .
                   â”‚     â”‚     â”‚     â””â”€â”€ pd_dataframe.pkl
                   â”‚     â”‚     â”œâ”€â”€ 0001/
                   â”‚     â”‚     â”œâ”€â”€ 0002/
                   â”‚     |     .
                   â”‚     |     â””â”€â”€ 0024/
                   â”‚     â”œâ”€â”€ Town03/
                   â”‚     â”œâ”€â”€ Town04/
                   â”‚     .
                   â”‚     â””â”€â”€ Town06/
                   â”œâ”€â”€ val0/
                   .
                   â””â”€â”€ val1/
```

## ğŸ¯ å¼‚å¸¸æ£€æµ‹è®­ç»ƒ (Anomaly Detection Training)

### å¿«é€Ÿå¼€å§‹ (Quick Start)
```bash
# ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆå¼‚å¸¸æ£€æµ‹é…ç½®
python train.py --config-file muvo/configs/anomaly_detection.yml
```

### é…ç½®è¯´æ˜ (Configuration)
- **é…ç½®æ–‡ä»¶**: `muvo/configs/anomaly_detection.yml`
- **æ ¸å¿ƒå‚æ•°**:
  - `MODEL.ANOMALY_DETECTION.ENABLED: True` - å¯ç”¨å¼‚å¸¸æ£€æµ‹
  - `MODEL.ANOMALY_DETECTION.FREEZE_BACKBONE: True` - å†»ç»“éª¨å¹²ç½‘ç»œ
  - `MODEL.ANOMALY_DETECTION.HEAD_TYPE: '3dcnn'` - å¼‚å¸¸æ£€æµ‹å¤´ç±»å‹
  - `MODEL.ANOMALY_DETECTION.HIDDEN_DIM: 128` - æ³¨æ„åŠ›éšè—ç»´åº¦
  - `MODEL.ANOMALY_DETECTION.NUM_HEADS: 8` - æ³¨æ„åŠ›å¤´æ•°

### è®­ç»ƒç‰¹ç‚¹ (Training Features)
- **é«˜æ•ˆè®­ç»ƒ**: éª¨å¹²ç½‘ç»œæƒé‡å†»ç»“ï¼Œä»…è®­ç»ƒå¼‚å¸¸æ£€æµ‹ç›¸å…³æ¨¡å—
- **ä½æ˜¾å­˜éœ€æ±‚**: ç›¸æ¯”ç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ˜¾å­˜éœ€æ±‚å¤§å¹…é™ä½
- **å¿«é€Ÿæ”¶æ•›**: ç”±äºéª¨å¹²ç½‘ç»œé¢„è®­ç»ƒï¼Œè®­ç»ƒæ”¶æ•›æ›´å¿«

## åŸå§‹è®­ç»ƒ (Original Training)
Run
```angular2html
python train.py --conifg-file muvo/configs/your_config.yml
```
You can use default config file `muvo/configs/muvo.yml`, or create your own config file in `muvo/configs/`. <br/>
In `config file(*.yml)`, you can set all the configs listed in `muvo/config.py`. <br/>
Before training, make sure that the required input/output data as well as the model structure/dimensions are correctly set in `muvo/configs/your_config.yml`.

## test

### weights

We provide weights for pre-trained models, and each was trained with around 100,000 steps. [weights](https://github.com/daniel-bogdoll/MUVO/releases/tag/1.0) is for a 1D latent space. [weights_2D](https://github.com/daniel-bogdoll/MUVO/releases/tag/2.0) for a 2D latent space. We provide config files for each:  <br/>  <br/> 
'basic_voxel' in [weights_2D](https://github.com/daniel-bogdoll/MUVO/releases/tag/2.0) is for the basic 2D latent space model, which uses resnet18 as the backbone, without bev mapping for image features, uses range view for point cloud and uses the transformer to fuse features, the corresponding config file is '[test_base_2d.yml](https://github.com/daniel-bogdoll/MUVO/blob/main/muvo/configs/test_base_2d.yml)';  <br/>  <br/> 
'mobilevit' weights just change the backbone compared to the 'basic_voxel' weights, the corresponding config file is '[test_mobilevit_2d.yml](https://github.com/daniel-bogdoll/MUVO/blob/main/muvo/configs/test_mobilevit_2d.yml)'; <br/>  <br/> 
'RV_WOB_TR_1d_Voxel' and 'RV_WOB_TR_1d_no_Voxel' in [weights](https://github.com/daniel-bogdoll/MUVO/releases/tag/1.0) all use basic setting but use 1d latent space, '[test_base_1d.yml](https://github.com/daniel-bogdoll/MUVO/blob/main/muvo/configs/test_base_1d.yml)' and '[test_base_1d_without_voxel.yml](https://github.com/daniel-bogdoll/MUVO/blob/main/muvo/configs/test_base_1d_without_voxel.yml)' are corresponding config files.

### execute
Run
```angular2html
python prediction.py --config-file muvo/configs/test.yml
```
The config file is the same as in training.\
In `file 'muvo/data/dataset.py', class 'DataModule', function 'setup'`, you can change the test dataset/sampler type.

## ğŸ—ï¸ æ¨¡å‹æ¶æ„è¯¦è§£ (Model Architecture Details)

### è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆæ¨¡å— (Cross-Modal Attention Fusion Module)
```python
# æ ¸å¿ƒç»„ä»¶
from muvo.models.cross_modal_attention import CrossModalFusionModule
from muvo.models.anomaly_detection_head import AnomalyDetectionHead

# ä½¿ç”¨ç¤ºä¾‹
cross_modal_fusion = CrossModalFusionModule(
    pc_feature_dim=256,
    img_feature_dim=256, 
    hidden_dim=128,
    num_heads=8
)

anomaly_head = AnomalyDetectionHead(
    input_dim=256,
    head_type='3dcnn',  # æˆ– 'mlp', 'multiscale'
    output_dim=1
)
```

### å¼‚å¸¸æ£€æµ‹æ¨¡å‹ (Anomaly Detection Model)
```python
# ä½¿ç”¨ä¿®æ”¹åçš„Mileæ¨¡å‹
from muvo.models.mile_anomaly import MileAnomalyDetection

model = MileAnomalyDetection(cfg)
model.freeze_backbone_weights()  # å†»ç»“éª¨å¹²ç½‘ç»œæƒé‡
```

### è®­ç»ƒå‚æ•°é…ç½® (Training Parameters)
```yaml
# å…³é”®é…ç½®å‚æ•°
MODEL:
  ANOMALY_DETECTION:
    ENABLED: True
    FREEZE_BACKBONE: True
    HEAD_TYPE: '3dcnn'
    HIDDEN_DIM: 128
    NUM_HEADS: 8
    DROPOUT: 0.1

OPTIMIZER:
  LR: 1e-3  # å¼‚å¸¸æ£€æµ‹å¤´å¯ä»¥ä½¿ç”¨è¾ƒé«˜å­¦ä¹ ç‡
  FROZEN:
    ENABLED: True
    TRAIN_LIST: ['cross_modal_fusion', 'anomaly_detection_head']
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŠ¿ (Performance Advantages)

### è®­ç»ƒæ•ˆç‡ (Training Efficiency)
- **æ˜¾å­˜ä½¿ç”¨**: ç›¸æ¯”ç«¯åˆ°ç«¯è®­ç»ƒå‡å°‘60-80%
- **è®­ç»ƒæ—¶é—´**: ç”±äºéª¨å¹²ç½‘ç»œå†»ç»“ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘50-70%
- **æ”¶æ•›é€Ÿåº¦**: é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œåŠ é€Ÿæ”¶æ•›

### æ£€æµ‹æ€§èƒ½ (Detection Performance)
- **è·¨æ¨¡æ€èåˆ**: å›¾åƒå’Œç‚¹äº‘ç‰¹å¾çš„æ·±åº¦äº¤äº’æå‡æ£€æµ‹ç²¾åº¦
- **å¤šå°ºåº¦æ£€æµ‹**: æ”¯æŒä¸åŒå°ºåº¦çš„å¼‚å¸¸æ£€æµ‹
- **å®æ—¶æ€§**: è½»é‡çº§è®¾è®¡é€‚åˆå®æ—¶åº”ç”¨

## ğŸ”§ æ‰©å±•åŠŸèƒ½ (Extension Features)

### è‡ªå®šä¹‰å¼‚å¸¸æ£€æµ‹å¤´
```python
# åˆ›å»ºè‡ªå®šä¹‰æ£€æµ‹å¤´
class CustomAnomalyHead(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        # è‡ªå®šä¹‰æ¶æ„
        pass
```

### æ·»åŠ æ–°çš„èåˆç­–ç•¥
```python
# æ‰©å±•è·¨æ¨¡æ€èåˆæ¨¡å—
class EnhancedCrossModalFusion(CrossModalFusionModule):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # æ·»åŠ æ–°çš„èåˆç­–ç•¥
        pass
```

## Related Projects
Our code is based on [MILE](https://github.com/wayveai/mile). 
And thanks to [CARLA-Roach](https://github.com/zhejz/carla-roach) for making a gym wrapper around CARLA.
