#!/usr/bin/env python3
"""
è¯„ä¼°Focal Lossè®­ç»ƒçš„æ¨¡å‹
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import sys
import os
import numpy as np
from tqdm import tqdm
from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, confusion_matrix
import matplotlib.pyplot as plt

sys.path.insert(0, '/root/autodl-tmp/MUVO/MUVO')

from muvo.dataset.anovox_dataset import AnoVoxDataset, collate_fn
from train_with_focal_loss import ImprovedAnomalyDetector, create_pseudo_labels


def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    all_scores = []
    all_labels = []
    all_predictions = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc='Evaluating'):
            batch['image'] = batch['image'].to(device)
            batch['points'] = batch['points'].to(device)
            
            labels = create_pseudo_labels(batch).to(device)
            outputs = model(batch)
            
            scores = outputs['anomaly_score'].cpu().numpy()
            labels_np = labels.cpu().numpy()
            predictions = (scores > 0.5).astype(float)
            
            all_scores.extend(scores.tolist())
            all_labels.extend(labels_np.tolist())
            all_predictions.extend(predictions.tolist())
    
    return np.array(all_scores), np.array(all_labels), np.array(all_predictions)


def compute_metrics(scores, labels, predictions):
    """è®¡ç®—æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    accuracy = (predictions == labels).mean()
    
    # æ··æ·†çŸ©é˜µ
    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    # AUROCå’ŒAUPRC
    try:
        auroc = roc_auc_score(labels, scores)
        fpr, tpr, _ = roc_curve(labels, scores)
    except:
        auroc = 0.0
        fpr, tpr = None, None
    
    try:
        auprc = average_precision_score(labels, scores)
        precision_curve, recall_curve, _ = precision_recall_curve(labels, scores)
    except:
        auprc = 0.0
        precision_curve, recall_curve = None, None
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auroc': auroc,
        'auprc': auprc,
        'confusion_matrix': {'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)},
        'roc_curve': (fpr, tpr),
        'pr_curve': (precision_curve, recall_curve)
    }


def visualize_comparison(metrics_before, metrics_after, scores_after, labels_after):
    """å¯¹æ¯”å¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. æŒ‡æ ‡å¯¹æ¯”
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUROC', 'AUPRC']
    before_values = [
        metrics_before['accuracy'],
        metrics_before['precision'],
        metrics_before['recall'],
        metrics_before['f1'],
        metrics_before['auroc'],
        metrics_before['auprc']
    ]
    after_values = [
        metrics_after['accuracy'],
        metrics_after['precision'],
        metrics_after['recall'],
        metrics_after['f1'],
        metrics_after['auroc'],
        metrics_after['auprc']
    ]
    
    x = np.arange(len(metric_names))
    width = 0.35
    
    axes[0, 0].bar(x - width/2, before_values, width, label='Before (BCE)', alpha=0.7, color='red')
    axes[0, 0].bar(x + width/2, after_values, width, label='After (Focal)', alpha=0.7, color='green')
    axes[0, 0].set_ylabel('Score')
    axes[0, 0].set_title('Metrics Comparison')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(metric_names, rotation=45)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    axes[0, 0].set_ylim([0, 1])
    
    # 2. ROCæ›²çº¿å¯¹æ¯”
    if metrics_before['roc_curve'][0] is not None:
        fpr_b, tpr_b = metrics_before['roc_curve']
        axes[0, 1].plot(fpr_b, tpr_b, 'r-', linewidth=2, label=f'Before (AUROC={metrics_before["auroc"]:.3f})')
    if metrics_after['roc_curve'][0] is not None:
        fpr_a, tpr_a = metrics_after['roc_curve']
        axes[0, 1].plot(fpr_a, tpr_a, 'g-', linewidth=2, label=f'After (AUROC={metrics_after["auroc"]:.3f})')
    axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
    axes[0, 1].set_xlabel('False Positive Rate')
    axes[0, 1].set_ylabel('True Positive Rate')
    axes[0, 1].set_title('ROC Curve Comparison')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æ··æ·†çŸ©é˜µå¯¹æ¯”
    cm_before = metrics_before['confusion_matrix']
    cm_after = metrics_after['confusion_matrix']
    
    cm_text = f"""
    Before (BCE Loss):
      TP: {cm_before['tp']:6d}  FP: {cm_before['fp']:6d}
      FN: {cm_before['fn']:6d}  TN: {cm_before['tn']:6d}
      Recall: {metrics_before['recall']:.2%}
    
    After (Focal Loss):
      TP: {cm_after['tp']:6d}  FP: {cm_after['fp']:6d}
      FN: {cm_after['fn']:6d}  TN: {cm_after['tn']:6d}
      Recall: {metrics_after['recall']:.2%}
    """
    axes[0, 2].text(0.1, 0.5, cm_text, fontsize=11, family='monospace', verticalalignment='center')
    axes[0, 2].axis('off')
    axes[0, 2].set_title('Confusion Matrix Comparison')
    
    # 4. åˆ†æ•°åˆ†å¸ƒ
    normal_scores = scores_after[labels_after == 0]
    anomaly_scores = scores_after[labels_after == 1]
    
    if len(normal_scores) > 0:
        axes[1, 0].hist(normal_scores, bins=50, alpha=0.6, color='green', label='Normal')
    if len(anomaly_scores) > 0:
        axes[1, 0].hist(anomaly_scores, bins=50, alpha=0.6, color='red', label='Anomaly')
    axes[1, 0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')
    axes[1, 0].set_xlabel('Anomaly Score')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Score Distribution (Focal Loss)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. åˆ†æ•°ç»Ÿè®¡
    stats_text = f"""
    Score Statistics:
    
    Normal samples:
      Mean: {normal_scores.mean():.4f}
      Std:  {normal_scores.std():.4f}
      Min:  {normal_scores.min():.4f}
      Max:  {normal_scores.max():.4f}
    
    Anomaly samples:
      Mean: {anomaly_scores.mean():.4f}
      Std:  {anomaly_scores.std():.4f}
      Min:  {anomaly_scores.min():.4f}
      Max:  {anomaly_scores.max():.4f}
    
    Separation:
      Delta Mean: {abs(anomaly_scores.mean() - normal_scores.mean()):.4f}
    """
    axes[1, 1].text(0.1, 0.5, stats_text, fontsize=10, family='monospace', verticalalignment='center')
    axes[1, 1].axis('off')
    axes[1, 1].set_title('Score Statistics')
    
    # 6. å…³é”®é—®é¢˜è¯Šæ–­
    diagnosis_text = f"""
    Diagnosis:
    
    Problem: {"SAME AS BEFORE! æ¨¡å‹ä»åœ¨æŠ•æœºå–å·§!" if metrics_after['auroc'] < 0.55 else "Improved!"}
    
    Evidence:
      1. TP = {cm_after['tp']} {"âŒ (Still 0!)" if cm_after['tp'] == 0 else "âœ…"}
      2. Recall = {metrics_after['recall']:.1%} {"âŒ" if metrics_after['recall'] < 0.05 else "âœ…"}
      3. AUROC = {metrics_after['auroc']:.3f} {"âŒ (Random!)" if metrics_after['auroc'] < 0.55 else "âœ…"}
    
    Root Cause:
      {"1. Focal Losså‚æ•°å¯èƒ½ä¸å¤Ÿæ¿€è¿›" if cm_after['tp'] == 0 else ""}
      {"2. ä¼ªæ ‡ç­¾è´¨é‡å¤ªå·®" if cm_after['tp'] == 0 else ""}
      {"3. éœ€è¦æ›´å¼ºçš„ç›‘ç£ä¿¡å·" if cm_after['tp'] == 0 else ""}
    
    Next Steps:
      {"- å¢å¤§alpha (0.25â†’0.1)" if cm_after['tp'] == 0 else ""}
      {"- å¢å¤§gamma (2.0â†’3.0)" if cm_after['tp'] == 0 else ""}
      {"- ä½¿ç”¨ä½“ç´ çº§æ ‡ç­¾" if cm_after['tp'] == 0 else ""}
      {"- åŠ å¤§å¼‚å¸¸æ ·æœ¬æƒé‡" if cm_after['tp'] == 0 else ""}
    """
    axes[1, 2].text(0.05, 0.5, diagnosis_text, fontsize=9, family='monospace', verticalalignment='center')
    axes[1, 2].axis('off')
    axes[1, 2].set_title('Diagnosis & Next Steps', color='red' if metrics_after['auroc'] < 0.55 else 'green')
    
    plt.tight_layout()
    plt.savefig('visualizations/focal_loss_evaluation.png', dpi=150, bbox_inches='tight')
    print("âœ… å¯¹æ¯”å¯è§†åŒ–ä¿å­˜è‡³: visualizations/focal_loss_evaluation.png")


def main():
    print("=" * 70)
    print("ğŸ“Š Focal Lossæ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“ åŠ è½½æ•°æ®...")
    dataset = AnoVoxDataset(
        data_root='/root/autodl-tmp/datasets/AnoVox/AnoVox_Dynamic_Mono_Town07',
        split='train'
    )
    dataloader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=False,
        num_workers=4,
        collate_fn=collate_fn
    )
    
    # åŠ è½½Focal Lossæ¨¡å‹
    print("\nğŸ—ï¸ åŠ è½½Focal Lossæ¨¡å‹...")
    model = ImprovedAnomalyDetector(freeze_backbone=True).to(device)
    
    if os.path.exists('checkpoints/focal_loss_final.pth'):
        checkpoint = torch.load('checkpoints/focal_loss_final.pth', map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… åŠ è½½æœ€ç»ˆæ¨¡å‹")
    else:
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # è¯„ä¼°
    print("\nğŸ” è¯„ä¼°Focal Lossæ¨¡å‹...")
    scores_after, labels_after, predictions_after = evaluate_model(model, dataloader, device)
    metrics_after = compute_metrics(scores_after, labels_after, predictions_after)
    
    print("\nâœ… Focal Lossæ¨¡å‹ç»“æœ:")
    print(f"   Accuracy:  {metrics_after['accuracy']:.4f}")
    print(f"   Precision: {metrics_after['precision']:.4f}")
    print(f"   Recall:    {metrics_after['recall']:.4f}")
    print(f"   F1:        {metrics_after['f1']:.4f}")
    print(f"   AUROC:     {metrics_after['auroc']:.4f}")
    print(f"   AUPRC:     {metrics_after['auprc']:.4f}")
    print(f"\n   Confusion Matrix:")
    print(f"   TP: {metrics_after['confusion_matrix']['tp']:6d}  FP: {metrics_after['confusion_matrix']['fp']:6d}")
    print(f"   FN: {metrics_after['confusion_matrix']['fn']:6d}  TN: {metrics_after['confusion_matrix']['tn']:6d}")
    
    # ä¹‹å‰çš„BCEæ¨¡å‹ç»“æœ(ä»æ—¥å¿—ä¸­)
    print("\nğŸ“Š ä¹‹å‰çš„BCE Lossæ¨¡å‹ç»“æœ:")
    metrics_before = {
        'accuracy': 0.9012,
        'precision': 0.0,
        'recall': 0.0,
        'f1': 0.0,
        'auroc': 0.4979,
        'auprc': 0.0983,
        'confusion_matrix': {'tp': 0, 'fp': 0, 'tn': 3780, 'fn': 420},
        'roc_curve': (None, None),
        'pr_curve': (None, None)
    }
    print(f"   Accuracy:  {metrics_before['accuracy']:.4f}")
    print(f"   Precision: {metrics_before['precision']:.4f}")
    print(f"   Recall:    {metrics_before['recall']:.4f}")
    print(f"   F1:        {metrics_before['f1']:.4f}")
    print(f"   AUROC:     {metrics_before['auroc']:.4f}")
    print(f"   AUPRC:     {metrics_before['auprc']:.4f}")
    
    # å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 70)
    print("ğŸ“ˆ å¯¹æ¯”åˆ†æ")
    print("=" * 70)
    
    auroc_change = metrics_after['auroc'] - metrics_before['auroc']
    recall_change = metrics_after['recall'] - metrics_before['recall']
    
    print(f"\nå…³é”®æŒ‡æ ‡å˜åŒ–:")
    print(f"  AUROC:  {metrics_before['auroc']:.4f} â†’ {metrics_after['auroc']:.4f} ({auroc_change:+.4f})")
    print(f"  Recall: {metrics_before['recall']:.4f} â†’ {metrics_after['recall']:.4f} ({recall_change:+.4f})")
    print(f"  TP:     {metrics_before['confusion_matrix']['tp']} â†’ {metrics_after['confusion_matrix']['tp']}")
    
    if metrics_after['auroc'] < 0.55 and metrics_after['confusion_matrix']['tp'] == 0:
        print("\nâŒ ç»“è®º: Focal Lossæœªèƒ½è§£å†³é—®é¢˜ï¼")
        print("   æ¨¡å‹ä»åœ¨'æŠ•æœºå–å·§' - æ°¸è¿œé¢„æµ‹æ­£å¸¸")
        print("\næ ¹æœ¬åŸå› :")
        print("   1. ä¼ªæ ‡ç­¾è´¨é‡å¤ªå·®ï¼ˆåœºæ™¯çº§ vs ä½“ç´ çº§ï¼‰")
        print("   2. Focal Losså‚æ•°å¯èƒ½ä¸å¤Ÿæ¿€è¿›")
        print("   3. éœ€è¦æ›´å¼ºçš„ç›‘ç£ä¿¡å·")
    elif metrics_after['auroc'] > 0.60:
        print("\nâœ… ç»“è®º: Focal Lossæœ‰æ•ˆï¼")
        print("   æ¨¡å‹å¼€å§‹çœŸæ­£å­¦ä¹ å¼‚å¸¸ç‰¹å¾")
    else:
        print("\nâš ï¸ ç»“è®º: Focal Lossæœ‰è½»å¾®æ”¹å–„")
        print("   ä½†æ•ˆæœä¸å¤Ÿæ˜¾è‘—ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # å¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
    visualize_comparison(metrics_before, metrics_after, scores_after, labels_after)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    main()

